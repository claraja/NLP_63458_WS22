\chapter{Zusammenfassung und Ausblick}
\label{ch:zusammenfassung}


------

- Zusammenfassung der Arbeit und der Ergebnisse (kritisch betrachtet) und Ausblick auf weiterführende Fragestellungen zum Thema.


- Eine Bewertung der Ergebnisse und der Bedeutung der Ergebnisse im wissenschaftlichen Umfeld.


- Offen Fragen sollten benannt werden und bieten die Gelegenheit, einen Ausblick auf sinnvolle weiterführende Arbeiten zu geben.

-----

In diesem Kapitel wird die Praktikumsarbeit zusammengefasst und ein Ausblick auf sinnvolle weiterführende Arbeiten gegeben.




\section{Zusammenfassung}
\label{sec:Zusammenfassung} 

In der vorliegenden Arbeit wurde die prototypische \emph{MedExtractor}-Software entwickelt, mit der automatisiert Wissensrepräsentationen englischsprachiger medizinischer Texte erstellt werden können. Diese Wissensrepräsentationen sollen dabei als Vorarbeit für einen Chatbot dienen, der Menschen mit psychischen Problemen beraten können soll. Im Rahmen dieses Praktikumsbeitrages beschränken sich die Wissensrepräsentationen auf die Erstellung einer Liste von psychologischen Krankheiten und ihren Symptomen. 

Die prototypische Software wurde in Python geschrieben und verwendet \emph{spaCy} als zentrale auf \emph{Natural Language Processing (NLP)} spezialisierte Programmbibliothek. Der Schwerpunkt lag dabei auf der Einbindung einer \emph{Named Entity Recognition (NER)}-Komponente zur Erkennung und Kategorisierung von medizinischen Fachbegriffen.

Als Quelle für medizinische Fachbegriffe diente das \emph{MetaMapLite}-Projekt der \emph{National Library of Medicine}, das Tausende von Einträgen enthält und insbesondere zahlreiche Begriffe als \emph{disorder/disease} bzw. \emph{finding} kategorisiert und damit sehr einfach für das Training einer \emph{NER}-Komponente verwendet werden kann. Als \emph{NER}-Komponente von \emph{spaCy} wurde der \emph{Entity Ruler} ausgewählt, da dieser konventionell die trainierten Begriffe in einem Text sucht und kein statistisches mit (zu Beginn des Praktikums nicht vorhandenen) Beispielsätzen trainiertes Modell verwendet.

Die prototypische Software enthält drei zentrale Module. Zum einen ist dies ein \emph{Präprozessor}, der den zu analysierenden Text vorstrukturiert, so dass dieser einfacher analysiert werden kann - z.B. durch Umwandlung von Aufzählungen in eine Reihe von Sätzen, die für sich allein stehen können. In dem so vorbereiteten Text sucht anschließend das Hauptmodul der Software, der mit dem medizinischem Fachvokabular vortrainierte \emph{KnowledgeExtractor}, nach Symptomen und Krankheiten und bringt diese in einen Zusammenhang, wenn sie in demselben Satz gefunden werden, und speichert diese in einer programminternen Wissensbasis ab. Abschließend serialisiert das \emph{RDFSerializer}-Modul die Wissensrepräsentation in Form einer \emph{Resource Description Frameworks (RDF)}.

Der \emph{MedExtractor} wurde mit zahlreichen Texten getestet und erzielte in mehreren Texten trotz seiner sehr einfachen Logik eine sehr hohen Genauigkeit, d.h. die vom \emph{MedExtractor} gefundenen Zusammenhänge waren auch in manuell erstellten Wissensrepräsentationen der Texte enthalten. Etwas weniger erfolgreich war der \emph{MedExtractor} hinsichtlich der Vollständigkeit der gefundenen Zusammenhänge.


\section{Ausblick}
\label{sec:Ausblick} 